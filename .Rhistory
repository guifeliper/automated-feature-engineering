model <- tree(target ~ ., data, subset=i.training.set)
# Test model
predictions <- predict(model, data[-i.training.set,], type="class")
# Compute accuracy
confusion.matrix <- table(predictions, data$target[-i.training.set])
accuracy[ds,"pruned.dt"] <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
}
# Exploratory analysis of results
apply(accuracy, 2, mean)
plot(accuracy[,1], accuracy[,2])
dif <- accuracy[, 1] - accuracy[, 2]
plot(dif)
hist(dif)
### DATASET CHARACTERIZATION ###
# 4. compute metafeatures
# Names of meta-features
data.characteristics <- c("n.examples", "n.attributes", "n.classes", "def.accuracy")
# Prepare matrix to store meta-features
meta.features <- matrix(NA, nrow=length(datasets), ncol=length(data.characteristics))
rownames(meta.features) <- datasets
colnames(meta.features) <- data.characteristics
# Process datasets
for (ds in datasets)
{
print(paste("Processing", ds, "..."))
# Read data
data <- read.table(paste(sep="", "./datasets/", ds, "/", ds, ".data"), sep=",", na.string="?")
# n.examples
meta.features[ds, "n.examples"] <- nrow(data)
# n.attributes
meta.features[ds, "n.attributes"] <- ncol(data) - 1
# n.classes
meta.features[ds, "n.classes"] <- nlevels(data[, ncol(data)])
# def.accuracy
class.frequencies <- table(data[, ncol(data)])
meta.features[ds, "def.accuracy"] <- max(class.frequencies) / nrow(data)
}
# Exploratory data analysis of metafeatures
# (you can further explore this by changing the column that is being plotted againts the difference in accuracy)
plot(meta.features[,1], dif)
### METALEARNING ###
# 5. learn metalearning model
# Create metadataset
meta.target <- factor(accuracy[, 1] > accuracy[, 2])
meta.data <- data.frame(meta.features, meta.target)
i.meta.training.set <- sample(1:nrow(meta.data), 0.7 * nrow(meta.data))
# Induce model
meta.model <- tree(meta.target ~ ., meta.data[i.meta.training.set,], method="class")
# 6. evaluate model
## Estimate meta-accuracy of decision trees
# Test model
meta.predictions <- predict(meta.model, meta.data[-i.meta.training.set,], type="class")
# Compute accuracy
meta.confusion.matrix <- table(meta.predictions, meta.data$meta.target[-i.meta.training.set])
sum(diag(meta.confusion.matrix)) /sum(meta.confusion.matrix)
# Default meta.accuracy
table(meta.data$meta.target)/nrow(meta.data)
View(meta.features)
View(meta.features)
library(tree)
library(tree)
# Definition of datasets
datasets <- c(
"abalone",
"agaricus-lepiota",
"allbp",
"allhyper",
"allhypo",
"allrep",
"anneal",
"audiology",
"balance-scale",
#"balloons",
#"bands",
"breast-cancer-wisconsin",
"bupa",
"car",
"c_class_flares",
"cmc",
"crx",
"dis",
"echocardiogram_alive-at-one",
"echocardiogram_still-alive",
"ecoli",
"flag_animate",
"flag_black",
"flag_blue",
"flag_botright",
"flag_crescent",
"flag_gold",
"flag_green",
"flag_icon",
"flag_landmass",
"flag_language",
"flag_mainhue",
"flag_orange",
"flag_red",
"flag_religion",
"flag_text",
"flag_topleft",
"flag_triangle",
"flag_white",
"flag_zone",
"german",
"german-numeric",
"glass",
"haberman",
"hayes-roth",
"heart",
#"heart-disease_cleveland_new",
#"heart-disease_hungarian",
#"heart-disease_long-beach-va",
"heart-disease_processed_cleveland",
"heart-disease_processed_hungarian",
#"heart-disease_processed_switzerland",
"heart-disease_processed_va",
#"heart-disease_switzerland",
"hepatitis",
"horse-colic",
"horse-colic_outcome",
"house-votes_84",
"hypothyroid",
"imports_85",
"ionosphere",
"iris",
"kr-vs-kp",
#"lrs",
"lung-cancer",
"m_class_flares",
"mfeat",
"monks_1",
"monks_2",
"monks_3",
#"musk_clean1",
"new-thyroid",
"optdigits",
"o-ring-erosion-only",
"o-ring-erosion-or-blowby",
"page-blocks",
"pima-indians-diabetes",
"post-operative",
"promoters",
"quadrupeds",
"segmentation",
"sick",
"sick-euthyroid",
"sonar",
"soybean-large",
"soybean-small",
"spambase",
"SPECT",
"SPECTF",
"splice",
"tae",
"tic-tac-toe",
"vehicle",
"vowel",
"vowel-context",
"waveform21",
"waveform40",
"wdbc",
"wine",
"wpbc",
"x_class_flares",
"yeast",
"zoo")
# 3. estimate the error of a decision tree with and without pruning on all the datasets
algorithms <- c("dt", "pruned.dt")
# Prepare matrix to store results
accuracy <- matrix(NA, nrow=length(datasets), ncol=length(algorithms))
rownames(accuracy) <- datasets
colnames(accuracy) <- algorithms
# Ensure process is repeatable
set.seed(1)
# Process datasets
for (ds in datasets)
{
print(paste("Processing", ds, "..."))
# Read data
data <- read.table(paste(sep="", "./datasets/", ds, "/", ds, ".data"), sep=",", na.string="?")
nrow(data)
# Name target attribute (assumes last column)
colnames(data)[ncol(data)] <- "target"
## Prepare for hold-out estimation
# Determine samples
i.training.set <- sample(1:nrow(data), 0.7 * nrow(data))
## Estimate accuracy of decision trees without pruning
# Induce model
model <- tree(target ~ ., data, subset=i.training.set, control=list(nobs=length(i.training.set) + 1, nmax=length(i.training.set), mindev=0))
# Test model
predictions <- predict(model, data[-i.training.set,], type="class")
# Compute accuracy
confusion.matrix <- table(predictions, data$target[-i.training.set])
accuracy[ds,"dt"] <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
## Estimate accuracy of pruned decision trees
# Induce model
model <- tree(target ~ ., data, subset=i.training.set)
# Test model
predictions <- predict(model, data[-i.training.set,], type="class")
# Compute accuracy
confusion.matrix <- table(predictions, data$target[-i.training.set])
accuracy[ds,"pruned.dt"] <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
}
# Exploratory analysis of results
apply(accuracy, 2, mean)
plot(accuracy[,1], accuracy[,2])
dif <- accuracy[, 1] - accuracy[, 2]
plot(dif)
hist(dif)
# Names of meta-features
data.characteristics <- c("n.examples", "n.attributes", "n.classes", "def.accuracy")
# Prepare matrix to store meta-features
meta.features <- matrix(NA, nrow=length(datasets), ncol=length(data.characteristics))
rownames(meta.features) <- datasets
colnames(meta.features) <- data.characteristics
# Process datasets
for (ds in datasets)
{
print(paste("Processing", ds, "..."))
# Read data
data <- read.table(paste(sep="", "./datasets/", ds, "/", ds, ".data"), sep=",", na.string="?")
# n.examples
meta.features[ds, "n.examples"] <- nrow(data)
# n.attributes
meta.features[ds, "n.attributes"] <- ncol(data) - 1
# n.classes
meta.features[ds, "n.classes"] <- nlevels(data[, ncol(data)])
# def.accuracy
class.frequencies <- table(data[, ncol(data)])
meta.features[ds, "def.accuracy"] <- max(class.frequencies) / nrow(data)
}
# Exploratory data analysis of metafeatures
# (you can further explore this by changing the column that is being plotted againts the difference in accuracy)
plot(meta.features[,1], dif)
# Create metadataset
meta.target <- factor(accuracy[, 1] > accuracy[, 2])
meta.data <- data.frame(meta.features, meta.target)
i.meta.training.set <- sample(1:nrow(meta.data), 0.7 * nrow(meta.data))
# Induce model
meta.model <- tree(meta.target ~ ., meta.data[i.meta.training.set,], method="class")
## Estimate meta-accuracy of decision trees
# Test model
meta.predictions <- predict(meta.model, meta.data[-i.meta.training.set,], type="class")
# Compute accuracy
meta.confusion.matrix <- table(meta.predictions, meta.data$meta.target[-i.meta.training.set])
sum(diag(meta.confusion.matrix)) /sum(meta.confusion.matrix)
# Default meta.accuracy
table(meta.data$meta.target)/nrow(meta.data)
# Default meta.accuracy
table(meta.data$meta.target)/nrow(meta.data)
View(meta.data)
library(mfe)
library(datasets)
data(iris)
summary(iris)
## Extract all measures using data frame
iris.info <- metafeatures(iris[,1:4], iris[,5])
install.packages("mfe")
library(mfe)
library(datasets)
data(iris)
summary(iris)
## Extract all measures using data frame
iris.info <- metafeatures(iris[,1:4], iris[,5])
iris.info
library(mfe)
library(datasets)
data(iris)
summary(iris)
## Extract all measures using data frame
iris.info <- metafeatures(iris[,1:4], iris[,5])
iris.info
load("~/Thesis/Docker Python/python/data/electricity.csv")
metadata <- read.csv(paste(sep="", "../data/history/Filter Original vs Extended/analiseNivelBase.csv"), header=TRUE, sep=",", row.names = 1)
metadata <- read.csv(paste(sep="", "../data/history/Filter Original vs Extended/analiseNivelBase.csv"), header=TRUE, sep=",", row.names = 1)
metadata_analise <- read.csv("~/Thesis/Docker Python/python/data/metadata_analise.csv")
View(metadata_analise)
View(metadata_analise)
sumary(metadata_analise)
summary(metadata_analise)
analiseNivelBase <- read.csv("~/Downloads/analiseNivelBase.csv")
View(analiseNivelBase)
clear
summary(analiseNivelBase)
Filter_analiseNivelBase <- read.csv("~/Thesis/Docker Python/python/data/history/Filter Original vs Extended/analiseNivelBase.csv")
View(Filter_analiseNivelBase)
analiseNivelBase <- read.csv("~/Thesis/Docker Python/python/data/history/Original Vs Extended/analiseNivelBase.csv")
View(analiseNivelBase)
summary(analiseNivelBase)
summary(Filter_analiseNivelBase)
test <- read.csv("~/Thesis/Docker Python/python/data/history/Original Vs Extended/test.csv")
View(test)
summary(test)
test <- read.csv("~/Thesis/Docker Python/python/data/history/Original Vs Extended/test.csv")
View(test)
summary(test)
analiseBase <- read.csv("~/Thesis/Docker Python/python/data/history/Original Vs Extended/analiseBase.csv")
View(analiseBase)
summary(analiseBase)
summary(analiseBase)
load("~/Thesis/Docker Python/python/Feature Selection/aids_test.csv")
aids_test <- read.csv("~/Thesis/Docker Python/python/Feature Selection/aids_test.csv")
View(aids_test)
d <- aids_test
d.features <- subset(d, select=-c(class))
d.labels <- d[,'class']
d.info <- metafeatures(d.features, d.labels, groups=c("general"))
library(mfe)
d.info <- metafeatures(d.features, d.labels, groups=c("general"))
d.info
d.info <- metafeatures(d.features, d.labels, groups=c("general", "statistical", "model.based"))
d.info
type <- 'extended'
i = 1
for (info in d.info)
{
name = paste0(type,".", names(d.info)[i])
metadata[c(ds), c(name)] <- info
i = i + 1
}
metadata <- read.csv(paste(sep="", "./metadata_general2.csv"), header=TRUE, sep=",", row.names = 1)
metadata <- read.csv(paste(sep="", "./metadata_general.csv"), header=TRUE, sep=",", row.names = 1)
summary(test)
metadata_accuracy <- read.csv("~/Thesis/Docker Python/python/data/history/Filter Original vs Extended/metadata_accuracy.csv")
View(metadata_accuracy)
summary(metadata_accuracy)
metadata_accuracy <- read.csv("~/Thesis/Docker Python/python/data/history/Original Vs Extended/metadata_accuracy.csv")
View(metadata_accuracy)
summary(metadata_accuracy)
library(mfe)
ls.metafeatures()
load("~/Thesis/Docker Python/python/Analysis/Original Vs Extended/metadata_base_general_no_correlation.csv")
metadata_general_no_correlation <- read.csv("~/Thesis/Docker Python/python/Analysis/Original Vs Extended/metadata_general_no_correlation.csv")
View(metadata_general_no_correlation)
View(metadata_general_no_correlation)
View(metadata_general_no_correlation)
summar´ry()
summary(metadata_general_no_correlation)
summary(metadata_general_no_correlation['class'])
#diff_a_b is a vector of differences between the two classifiers, on each fold of cross-validation.
#If you have done 10 runs of 10-folds cross-validation, you have 100 results for each classifier.
#You should have run cross-validation on the same folds for the two classifiers.
#Then diff_a_b is the difference fold-by-fold.
#rho is the correlation of the cross-validation results: 1/(number of folds)
#rope_min and rope_max are the lower and the upper bound of the rope
correlatedBayesianTtest <- function(diff_a_b,rho,rope_min,rope_max){
if (rope_max < rope_min){
stop("rope_max should be larger than rope_min")
}
delta <- mean(diff_a_b)
n <- length(diff_a_b)
df <- n-1
stdX <- sd(diff_a_b)
sp <- sd(diff_a_b)*sqrt(1/n + rho/(1-rho))
p.left <- pt((rope_min - delta)/sp, df)
p.rope <- pt((rope_max - delta)/sp, df)-p.left
results <- list('left'=p.left,'rope'=p.rope,'right'=1-p.left-p.rope)
return (results)
}
a <- c(0.60251665, 0.57752308, 0.68621431, 0.58682635, 0.75064157, 0.60082136, 0.21765469, 0.18188651, 0.35859729, 0.64319746) * 100
b <- c(0.3033848725449227, 0.4722106142916841, 0.481821980777267, 0.4600919348098621,
0.32636857501044714, 0.7145842039281237, 0.3422482239866277, 0.0, 0.2946092770580861, 0.4868366067697451) * 100
diff_a_b = a - b
correlatedBayesianTtest(diff_a_b, 1/10,-0.05, 0.05)
#diff_a_b is a vector of differences between the two classifiers, on each fold of cross-validation.
#If you have done 10 runs of 10-folds cross-validation, you have 100 results for each classifier.
#You should have run cross-validation on the same folds for the two classifiers.
#Then diff_a_b is the difference fold-by-fold.
#rho is the correlation of the cross-validation results: 1/(number of folds)
#rope_min and rope_max are the lower and the upper bound of the rope
correlatedBayesianTtest <- function(diff_a_b,rho,rope_min,rope_max){
if (rope_max < rope_min){
stop("rope_max should be larger than rope_min")
}
delta <- mean(diff_a_b)
n <- length(diff_a_b)
df <- n-1
stdX <- sd(diff_a_b)
sp <- sd(diff_a_b)*sqrt(1/n + rho/(1-rho))
p.left <- pt((rope_min - delta)/sp, df)
p.rope <- pt((rope_max - delta)/sp, df)-p.left
results <- list('left'=p.left,'rope'=p.rope,'right'=1-p.left-p.rope)
return (results)
}
# a <- c(0.60251665, 0.57752308, 0.68621431, 0.58682635, 0.75064157, 0.60082136, 0.21765469, 0.18188651, 0.35859729, 0.64319746) * 100
# b <- c(0.3033848725449227, 0.4722106142916841, 0.481821980777267, 0.4600919348098621,
#        0.32636857501044714, 0.7145842039281237, 0.3422482239866277, 0.0, 0.2946092770580861, 0.4868366067697451) * 100
a <- c(0.69859813, 0.53330706, 0.82488479, 0.63636364, 0.74644924, 0.73474291, 0.39273608, 0.5154013,  0.58105863, 0.30561634) * 100
b <- c(0.65186916, 0.53409539, 0.79597822, 0.73524721, 0.69016307, 0.2037482, 0.31912833, 0.41084599, 0.24471873, 0.30051058) * 100
diff_a_b = a - b
correlatedBayesianTtest(diff_a_b, 1/10,-0.05, 0.05)
#diff_a_b is a vector of differences between the two classifiers, on each fold of cross-validation.
#If you have done 10 runs of 10-folds cross-validation, you have 100 results for each classifier.
#You should have run cross-validation on the same folds for the two classifiers.
#Then diff_a_b is the difference fold-by-fold.
#rho is the correlation of the cross-validation results: 1/(number of folds)
#rope_min and rope_max are the lower and the upper bound of the rope
correlatedBayesianTtest <- function(diff_a_b,rho,rope_min,rope_max){
if (rope_max < rope_min){
stop("rope_max should be larger than rope_min")
}
delta <- mean(diff_a_b)
n <- length(diff_a_b)
df <- n-1
stdX <- sd(diff_a_b)
sp <- sd(diff_a_b)*sqrt(1/n + rho/(1-rho))
p.left <- pt((rope_min - delta)/sp, df)
p.rope <- pt((rope_max - delta)/sp, df)-p.left
results <- list('left'=p.left,'rope'=p.rope,'right'=1-p.left-p.rope)
return (results)
}
# a <- c(0.60251665, 0.57752308, 0.68621431, 0.58682635, 0.75064157, 0.60082136, 0.21765469, 0.18188651, 0.35859729, 0.64319746) * 100
# b <- c(0.3033848725449227, 0.4722106142916841, 0.481821980777267, 0.4600919348098621,
#        0.32636857501044714, 0.7145842039281237, 0.3422482239866277, 0.0, 0.2946092770580861, 0.4868366067697451) * 100
a <- c(0.69859813, 0.53330706, 0.82488479, 0.63636364, 0.74644924, 0.73474291, 0.39273608, 0.5154013,  0.58105863, 0.30561634) * 100
b <- c(0.65186916, 0.53409539, 0.79597822, 0.73524721, 0.69016307, 0.2037482, 0.31912833, 0.41084599, 0.24471873, 0.30051058) * 100
diff_a_b = a - b
correlatedBayesianTtest(diff_a_b, 1/10,-0.05, 0.05)
#diff_a_b is a vector of differences between the two classifiers, on each fold of cross-validation.
#If you have done 10 runs of 10-folds cross-validation, you have 100 results for each classifier.
#You should have run cross-validation on the same folds for the two classifiers.
#Then diff_a_b is the difference fold-by-fold.
#rho is the correlation of the cross-validation results: 1/(number of folds)
#rope_min and rope_max are the lower and the upper bound of the rope
correlatedBayesianTtest <- function(diff_a_b,rho,rope_min,rope_max){
if (rope_max < rope_min){
stop("rope_max should be larger than rope_min")
}
delta <- mean(diff_a_b)
n <- length(diff_a_b)
df <- n-1
stdX <- sd(diff_a_b)
sp <- sd(diff_a_b)*sqrt(1/n + rho/(1-rho))
p.left <- pt((rope_min - delta)/sp, df)
p.rope <- pt((rope_max - delta)/sp, df)-p.left
results <- list('left'=p.left,'rope'=p.rope,'right'=1-p.left-p.rope)
return (results)
}
# a <- c(0.60251665, 0.57752308, 0.68621431, 0.58682635, 0.75064157, 0.60082136, 0.21765469, 0.18188651, 0.35859729, 0.64319746) * 100
# b <- c(0.3033848725449227, 0.4722106142916841, 0.481821980777267, 0.4600919348098621,
#        0.32636857501044714, 0.7145842039281237, 0.3422482239866277, 0.0, 0.2946092770580861, 0.4868366067697451) * 100
a <- c(0.69859813, 0.53330706, 0.82488479, 0.63636364, 0.74644924, 0.73474291, 0.39273608, 0.5154013,  0.58105863, 0.30561634) * 100
b <- c(0.65186916, 0.53409539, 0.79597822, 0.73524721, 0.69016307, 0.2037482, 0.31912833, 0.41084599, 0.24471873, 0.30051058) * 100
diff_a_b = a - b
correlatedBayesianTtest(diff_a_b, 1/10,-0.1, 0.1)
#diff_a_b is a vector of differences between the two classifiers, on each fold of cross-validation.
#If you have done 10 runs of 10-folds cross-validation, you have 100 results for each classifier.
#You should have run cross-validation on the same folds for the two classifiers.
#Then diff_a_b is the difference fold-by-fold.
#rho is the correlation of the cross-validation results: 1/(number of folds)
#rope_min and rope_max are the lower and the upper bound of the rope
correlatedBayesianTtest <- function(diff_a_b,rho,rope_min,rope_max){
if (rope_max < rope_min){
stop("rope_max should be larger than rope_min")
}
delta <- mean(diff_a_b)
n <- length(diff_a_b)
df <- n-1
stdX <- sd(diff_a_b)
sp <- sd(diff_a_b)*sqrt(1/n + rho/(1-rho))
p.left <- pt((rope_min - delta)/sp, df)
p.rope <- pt((rope_max - delta)/sp, df)-p.left
results <- list('left'=p.left,'rope'=p.rope,'right'=1-p.left-p.rope)
return (results)
}
# a <- c(0.60251665, 0.57752308, 0.68621431, 0.58682635, 0.75064157, 0.60082136, 0.21765469, 0.18188651, 0.35859729, 0.64319746) * 100
# b <- c(0.3033848725449227, 0.4722106142916841, 0.481821980777267, 0.4600919348098621,
#        0.32636857501044714, 0.7145842039281237, 0.3422482239866277, 0.0, 0.2946092770580861, 0.4868366067697451) * 100
a <- c(0.69859813, 0.53330706, 0.82488479, 0.63636364, 0.74644924, 0.73474291, 0.39273608, 0.5154013,  0.58105863, 0.30561634) * 100
b <- c(0.65186916, 0.53409539, 0.79597822, 0.73524721, 0.69016307, 0.2037482, 0.31912833, 0.41084599, 0.24471873, 0.30051058) * 100
diff_a_b = a - b
correlatedBayesianTtest(diff_a_b, 1/10,-0.05, 0.05)
#diff_a_b is a vector of differences between the two classifiers, on each fold of cross-validation.
#If you have done 10 runs of 10-folds cross-validation, you have 100 results for each classifier.
#You should have run cross-validation on the same folds for the two classifiers.
#Then diff_a_b is the difference fold-by-fold.
#rho is the correlation of the cross-validation results: 1/(number of folds)
#rope_min and rope_max are the lower and the upper bound of the rope
correlatedBayesianTtest <- function(diff_a_b,rho,rope_min,rope_max){
if (rope_max < rope_min){
stop("rope_max should be larger than rope_min")
}
delta <- mean(diff_a_b)
n <- length(diff_a_b)
df <- n-1
stdX <- sd(diff_a_b)
sp <- sd(diff_a_b)*sqrt(1/n + rho/(1-rho))
p.left <- pt((rope_min - delta)/sp, df)
p.rope <- pt((rope_max - delta)/sp, df)-p.left
results <- list('left'=p.left,'rope'=p.rope,'right'=1-p.left-p.rope)
return (results)
}
a <- c(0.60251665, 0.57752308, 0.68621431, 0.58682635, 0.75064157, 0.60082136, 0.21765469, 0.18188651, 0.35859729, 0.64319746) * 100
b <- c(0.3033848725449227, 0.4722106142916841, 0.481821980777267, 0.4600919348098621,
0.32636857501044714, 0.7145842039281237, 0.3422482239866277, 0.0, 0.2946092770580861, 0.4868366067697451) * 100
# a <- c(0.69859813, 0.53330706, 0.82488479, 0.63636364, 0.74644924, 0.73474291, 0.39273608, 0.5154013,  0.58105863, 0.30561634) * 100
# b <- c(0.65186916, 0.53409539, 0.79597822, 0.73524721, 0.69016307, 0.2037482, 0.31912833, 0.41084599, 0.24471873, 0.30051058) * 100
diff_a_b = a - b
correlatedBayesianTtest(diff_a_b, 1/10,-0.05, 0.05)
# Definition of datasets
datasets <-  list.files('../data/base',full.names = FALSE)
# Definition of datasets
datasets <-  list.files('../data/base/',full.names = FALSE)
# Definition of datasets
datasets <-  list.files('../../data/base/',full.names = FALSE)
setwd("/Users/guifeliper/Thesis/autoML/automated-feature-engineering")
# Definition of datasets
datasets <-  list.files('../data/base/',full.names = FALSE)
# Definition of datasets
datasets <-  list.files('../../data/base/',full.names = FALSE)
# Definition of datasets
datasets <-  list.files('./data/base/',full.names = FALSE)
